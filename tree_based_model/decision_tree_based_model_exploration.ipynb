{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 什么是Decision Tree（决策树）？它是如何工作的？\n",
    "\n",
    "决策树是一种监督学习算法（也就是有预先定义的目标或标签变量），大多时候用在分类问题中。决策树的示意图如下图所示。它适用categirical和continuous输入和输出变量。\n",
    "<img src=\"./images/decision_tree.png\" width=70%, height=70% align=center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树例子🌰\n",
    "假设有一个包含**30个**学生的数据集，每名学生有三个特征：性别，班级，身高。其中有15个会打篮球🏀。现在我们要创建一个模型来预测谁会打篮球。我们将根据三个特征中**最重要**的特征来区分。\n",
    "\n",
    "决策树能够根据三个特征的所有取值将学生进行划分为不同的集合，每个集合中的元素（这里是学生）具有同质性，而不同集合中的元素是异构的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从下图可看出，较与另外两个特征，性别特征能够识别最高的同质化集合。\n",
    "![按性别划分](./images/gender_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面提到，决策树能够辨识出**最重要**的特征及其取值，从而能够给出具有最高同质化的分类集合。问题来了，**它是如何识别了改变量并进行划分的？**事实上，决策树有很多算法来实现这一目的，下文将会结合实例介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的类型\n",
    "\n",
    "根据*目标变量的类型*， 可以将其分成两类：\n",
    "1. **Categorical 变量决策树**： 即目标变量为categrocal类型，比如，在上述关于学生问题中，目标变量取值为是否会打篮球：yes or no。\n",
    "2. **Continuous 变量决策树**： 即目标变量为连续型。\n",
    "\n",
    "### 关于决策树类型的例子\n",
    "\n",
    "假设我们有个问题：预测顾客是否会继续购买保险（yes/no）。这里，我们容易想到，顾客的收入将会是**重要**变量，但保险公司并不知道所有顾客的收入详情。所以，我们可以构建一个决策树模型，根据顾客分职业，保险产品类型等特征来预测其收入。这中场景下，我们要预测的值为连续的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的重要术语\n",
    "\n",
    "1. 根结点（Root Node）：它代表所有的样本集合，将会被进一步分为两个或者多个同质化的集合。\n",
    "2. 划分（Split）： 指划分节点中的样本到子节点的过程\n",
    "3. 决策节点（Decision Node）：当一个子节点进一步划分出子节点，则称其为决策节点\n",
    "4. 叶子节点（Leaf Node）：不再进行划分的节点\n",
    "5. 剪枝（Pruning）： 当我们将一个决策节点分子节点删除时，这一过程为剪枝。它是划分的反向过程。\n",
    "6. 分支/子树（Branch/Sub-Tree）： 整棵树的子集\n",
    "7. 父节点和孩子节点（Parent and Child Node）：被划为出子节点的节点成为父节点，相应的子节点为孩子节点。\n",
    "\n",
    "![决策树节点定义](./images/Decision_Tree_Node_Def.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 决策树的优势\n",
    "1. **容易理解**：即便是对没有数据分析背景的人来说，决策树的输出是非常容易理解的，它不需要统计知识背景来解释，仅通过将决策树图形化表示即可容易地与用户的假设相关联起来。\n",
    "2. **在数据探索中非常有用**： 决策树是能够最快从多个变量中辨识最重要的变量以及变量间关系的方法之一。借助它，我们可以创建新的变量/特征来更有力地预测目标变量，可参考这篇文章([Trick to enhance power of regression model](https://www.analyticsvidhya.com/blog/2013/10/trick-enhance-power-regression-model-2/)来了解这一trick的更多信息。此外，若我们要解决的问题包含上百个特征，那么决策树可用来选择最重要的几个特征，\n",
    "3. **所需的数据清洗更少**： 相对其它模型而言，它在一定程度上容忍异常值和缺失值。\n",
    "4. **对数据类型不限制**： 它可处理数值型以及categorical的特征。\n",
    "5. **非参数方法**： 也就是说，对空间分布以及模型结构没有前提假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的劣势\n",
    "\n",
    "1. **容易过拟合**： 过拟合是决策树模型最现实的苦难之一。可通过对模型参数的设置限制以及剪枝方法来解决。\n",
    "2. **对连续型变量适应性不强**： 虽然能能够处理连续型的数值变量，但是当决策树在将这类变量离散化时会损失信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 分类树和回归树\n",
    "分类树和回归树工作方式非常相似，它们的主要不同与相似点总结如下：\n",
    "\n",
    "1. 当目标变量为连续型是，用回归树；当目标变量为categorical时，用分类树。\n",
    "2. 对于回归树来讲，叶子节点的取值为落入其范围的所有训练样本的响应的均值。因此，对于没有见过的数据实例（unseen data），用均值来预测它的值。\n",
    "3. 对分类树来讲，叶子节点的取值（即分类标签），即为落入其范围的训练数据的标签。因此，对unseen data，用叶子节点的标签来预测它的所属类别标签。\n",
    "4. 这两种树都将预测值的空间分成了不同的且非重叠的高维区域。\n",
    "5. 树分裂过程一直持续到用户指定的停止指标为止。\n",
    "6. 两种树都遵从自顶向下的贪心分裂策略。\n",
    "7. 两种树均容易过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. 决策树是如何决定划分的（split）？\n",
    "\n",
    "树的分裂策略严重影响决策树的准确性。对于分类树和回归树来说，决策的指标是不同的。\n",
    "\n",
    "决策树使用多种算法来决定将一个节点分裂为两个或多个节点。分裂后的子节点将会具有对目标变量更高的纯净度。决策树用所有可用的特征/变量来分裂节点，然后选择子节点具有最高同质性的划分。\n",
    "\n",
    "常用的算法有：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini 系数解释\n",
    "Gini系数含义是：从一个种群中随机挑选出两个样本，如果种群是纯净的，那么随机选择的两个样本属于同一类别，并且概率为1.\n",
    "\n",
    "1. Gini系统法对categorical的目标变量有效，目标变量的取值为“sucess”或者“failure”；\n",
    "2. 仅能进行binary划分\n",
    "3. Gini值越高，说明纯净度越高\n",
    "4. 分类回归树（Classification and Regression Tree, CART）使用Gini来创建binary划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Gini系数来划分的计算步骤为：\n",
    "\n",
    "1. 计算子节点的Gini系数，计算公式为：$p^2 + q^2$， p和q分别为success和failure的概率\n",
    "2. 用各节点的划分的加权Gini score来计算某一划分的Gini系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用Gini系数来划分🌰：\n",
    "![Gini系数划分实例](./images/Decision_Tree_Algorithm1_Gini.png)\n",
    "\n",
    "#### 用Gender划分：\n",
    "1. 计算Female的子节点的Gini系数：$0.2^2+0.8^2 = 0.68$\n",
    "2. Male子节点的Gini系数： $0.65^2+0.35^2 = 0.55$\n",
    "3. 计算采用Gender划分的加权Gini系数：$\\frac{10}{30}\\times 0.68 + （\\frac{20}{30}\\times0.55 = \\bf0.59$\n",
    "\n",
    "#### 类似地，用Class划分：\n",
    "1. 子节点Class IX： $0.43^2 + 0.57^2 = 0.51$\n",
    "2. 子节点Class X： $0.56^2 + 0.44^2 = 0.51$\n",
    "3. 用Class划分的加权Gini系数：$\\frac{14}{30} \\times 0.51 + \\frac{16}{30} \\times 0.51 = \\bf 0.51$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的计算可知，采用Gender划分的Gini系数高于用Class划分，因此，改节点的划分将采用Gender。如果有更多的特征，将采用上述类似的方法，继续划分下去，并且在进一步划分时，并不会考虑已经存在的划分，也就是上文提到的“贪心策略”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益（information Gain）\n",
    "先从直观上认识下信息增益的含义。观察下图并思考哪一个节点更容易描述出来。可以肯定，你的答案是C，因为它的取值是相似的，它需要更少的信息。另一方面，B需要更多的信息，而A需要最多的信息量。也就是说，C是最纯净的（pure），而B比A更纯净。\n",
    "![](./images/Information_Gain_Decision_Tree2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，可以下结论，越纯净的节点需要越少的信息来描述它，反之亦然。信息论中，可用“熵”（Entropy）来描述系统的混乱程度。如果一个样本是完全同质的，那么entropy为0，而如果一个样本是平均区分开的（50%-50%），那么entropy为1.\n",
    "\n",
    "entropy的定义为：\n",
    "$$Entropy = -p\\log_{2}p - q\\log_{2}q$$\n",
    "其中，p和q分别为在某一节点上“sucess”和“failure”的概率。Entropy同样适用于categorical目标变量。它会选择相对于父节点和其它划分取得最小entropy的划分。**entropy越小越好。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算Entropy来进行划分的步骤：\n",
    "\n",
    "1. 计算父节点的entropy\n",
    "2. 计算各划分中各独立节点的entropy并计算所有可用的子节点entropy的加权平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 采用计算entropy来对上述样本进行划分：\n",
    "1. 父节点的Entropy：$-\\frac{15}{30}\\times \\log_{2} \\frac{15}{30} - \\frac{15}{30}\\times \\log_{2} \\frac{15}{30} = \\bf1$.这里1表示为不纯净的点\n",
    "\n",
    "2. Female 节点的Entropy： $-\\frac{2}{10}\\times \\log_{2} \\frac{2}{10}-\\frac{8}{10}\\times \\log_{2} \\frac{8}{10}= \\bf 0.72$. 同时，对Male节点，$-\\frac{13}{20}\\times \\log_{2} \\frac{13}{20}-\\frac{7}{20}\\times \\log_{2} \\frac{7}{20} = \\bf 0.93$\n",
    "\n",
    "3. 采用Gender划分的entropy加权平均值为：$\\frac{10}{30} \\times 0.72 + \\frac{20}{30} \\times 0.93 = \\bf 0.86$\n",
    "\n",
    "4. Class IX节点的entropy：$-\\frac{6}{14}\\log_{2}\\frac{6}{14} - \\frac{8}{14}\\log_{2}\\frac{8}{14} = \\bf 0.99$. 同时，对Class X节点： $-\\frac{9}{16}\\log_{2}\\frac{9}{16} - \\frac{7}{16}\\log_{2}\\frac{7}{16} = \\bf 0.99$\n",
    "\n",
    "5. 采用Class划分的entropy的加权平均值为：$\\frac{14}{30} \\times 0.99 + \\frac{16}{30} \\times 0.99 = \\bf 0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上述计算结果表明，用Gender进行划分的entropy是最小的。我们可以推算出信息增益：$gain = 1-entropy$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 树模型的关键参数是哪些？如何避免over-fitting？\n",
    "\n",
    "overfitting是决策树模型面临的关键挑战。如果对其不加限制，决策树将会在训练数据上取得100%的准确率，因为它可以为单一样本生成一个叶子节点。因此，阻止过拟合是对决策树建模及其重要的，其中，主要有两类方法：\n",
    "1. 对树的规模加以限制\n",
    "2. 剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对树的规模进行限制\n",
    "这类方法可通过设置多个用来定义树的参数来限制树的规模。首先，我们看下图中决策树的结构：\n",
    "![](./images/tree-constraint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中的参数将被用来定义树的结构。理解这些参数很重要，并且，它们在python库中也是常用的。具体解释如下：\n",
    "1. 节点分裂的最小样本数（min_samples_split)。\n",
    "    - 定义节点上最小样本数，只有满足该数值才对该节点进行再划分\n",
    "    - 可用来控制过拟合。值越大，表示阻止模型学习到一些针对特殊样本的可能过于具体的关系\n",
    "    - 值过大可能导致under-fitting，一般情况下，可用cross validadion（CV）来调整这一参数\n",
    "2. 叶子节点上的最小样本数（min_samples_leaf)\n",
    "    - 定义叶子节点上所需的最小样本数\n",
    "    - 与min_samples_split类似，可用来防止过拟合\n",
    "    - 通常情况下，对于样本不均衡的分类问题，会选择比较小的值，因为对于样本少的类别所在的区间很小。\n",
    "3. 树的最大深度（max_depth）\n",
    "    - 定义树的最大深度\n",
    "    - 也可用于控制over-fitting：树深越高，允许模型学到样本间特别具体的关系的程度就越高\n",
    "    - 应用CV来调整\n",
    "4. 叶子节点的最大数量(max_leaf_nodes)\n",
    "    - 定义树中叶子节点最大数量\n",
    "    - 可用来代替max_depth， 因为对二叉树，树深为n，则最大叶子节点为$2^n$\n",
    "5. 用于划分的最大特征数量。\n",
    "    - 定义用来寻找最优划分的特征数量。特征是随机选择出来的。\n",
    "    - Thumb-rule：总特征数的平方根表象地很好，但应该考虑到30-40%的特征。\n",
    "    - 值越高越容易导致overfitting，但要根据具体case分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 剪枝\n",
    "剪枝方法也可以用来抑制overfitting，其基本思想是：\n",
    "1. 我们首先将树生长到很高的深度\n",
    "2. 然后从树的底部开始移除树枝\n",
    "3. 决定是否移除的指标是可根据信息增益的增减。\n",
    "目前sklearn中非决策树模型未支持剪枝，更advsnced的库，如xgboost采用了剪枝的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 决策树的实现\n",
    "sklearn采用CART算法来训练决策树（有时也叫“生长树”）。CART算法的思想很直观：用特征$k$和一个阈值$t_k$来将训练样本分成两个子集，选择$(k,t_k)$的方法是，就是要使得子集的纯净度最高，纯净度可通过Gini系数和信息熵来衡量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', \n",
    "                                  max_depth=2, \n",
    "                                  min_samples_leaf=2,\n",
    "                                  min_samples_split=2,\n",
    "                                  max_features=2)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "y_pred = tree_clf.predict(X)\n",
    "\n",
    "tree_clf.score(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Bagging 与随机森林\n",
    "\n",
    "随机森林属于集成学习方法中的一种，集成学习主要思想是，采用多种算法训练得到丰富的分类器，然后将它们的预测结果整合在一起，得到一个更好的预测结果。换句话说就是，“三个臭皮匠，顶个诸葛亮”。将多个（弱）分类器的预测结果整合的方法，最简单的就是通过多数投票和取平均。下图是采用多数投票的方法得到最终预测结果的示意图\n",
    "\n",
    "## Voting Classifier\n",
    "![image.png](./images/voting_classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有点让人吃惊的是，采用上述方法得到的最终分类器，准确度一般要比采用单个分类器准确率要高。在scikit-learn中也有对这种多数投票法实现集成学习的实现，即VotingClassifier()。下面是将Decision Tree，Logistics regression以及SVM采用多数投票的方法组成的分类器，从结果可看到，对该数据集，集成学习的分类准确度更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.635\n",
      "LogisticRegression 0.725\n",
      "SVC 0.725\n",
      "VotingClassifier 0.7266666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# 采用多个来自不同算法的分类模型以bagging的方式做ensemble learning：\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=3000, noise=0.9, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "lr_clf = LogisticRegression(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', lr_clf),\n",
    "                                          ('dt', dt_clf),\n",
    "                                          ('svm', svm_clf)],\n",
    "                             voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (dt_clf, lr_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "Bagging (short for bootstrap aggregating) 也是一种集成学习（ensemble learning）的技术。上面讲的VotingClassifier是用不同的算法得到的几个模型进行整合，事实上，我们可以用同一种算法在不同的训练集子集上训练得到不同的弱分类器，然后再将多个弱学习器的预测结果结合起来，从而降低预测误差。每个弱学习器是在训练集的子集上建立了。下图可以很形象的说明：\n",
    "\n",
    "![](./images/bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging通过对训练集进行有放回的采样来生成不同的子集，从而在各子集上训练不同的弱学习器，也就是说，对每个弱学习器来说，bagging允许对相同的样本采样多次。若每个弱学习器都可以对分类问题产生预测概率，那么bagging的最终预测概率为它们的加权平均，这叫“**soft voting**；若不能预测概率（如SVM 分类器），那么采用多数投票的方式来生成最终的预测结果，这叫**”hard voting“**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn集成了采用bootstr采样的bagging分类器，即BaggingClassifier()，以下是采用多个弱分类器（决策树）实现bagging方法的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                            n_estimators=500, \n",
    "                            max_samples=100, \n",
    "                            bootstrap=True, \n",
    "                            n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest（随机森林）被称作机器学习里“万金油”， 也是非常杰出的一个算法，它能处理分类和回归问题。另外，它也可用来做降维，缺失值处理，异常值处理等其它数据探索的任务，并且都有不错的效果。它是bagging集成学习的一种，将多个决策树模型组合起来形成一个强有力的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林如何工作的\n",
    "\n",
    "我们采用CART算法生成多个决策树，对于分类问题，每棵树根据样本的属性给出一个分类结果，然后随机森林选择具有最多投票结果的类别作为分类结果，对于回归问题，它将不同树的结果做平均作为预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每棵树的生成方式为：\n",
    "\n",
    "1. 假设训练集有M个样本，那么将从这M个样本中做有放回的随机抽样，抽样的子集将用来训练决策树。\n",
    "2. 若有N个特征，将选取n<N个特征用来寻找最优的划分\n",
    "3. 每棵树都以最大限度的生长，并且不做剪枝。\n",
    "4。 通过将各棵树的结果做汇聚（投票或者取平均）\n",
    "\n",
    "下图说明RF的训练方式：\n",
    "![](images/randm_forest_split.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest的实现\n",
    "虽然随机森林可以用BaggingClassifier()实现，但scikit-learn提供了更方便的RandomForestClassifier()，它对决策树算法做了更多的优化。同时除了几个少部分的参数外，RandomForestClassifier()几乎保留了与DesicionTreeClassifier()相同所有的超参数，来控制决策树的生长，另外还附加了BaggingClassifier()的参数来控制集成学习本身，以下是随机森林算法的实现示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林的示例代码\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林算法对决策树的生长增加了额外的随机性，它是从随机选择的特征子集中找到最优的节点划分，而不是像决策树一样，在所有的特征上来寻找最优的划分。这将进一步增加所得到的决策树的多样性。从误差的角度来讲，是以更高的bias来换取了更低variance。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest的优势\n",
    "1. 可用于分类和回归任务\n",
    "2. 一个胜过绝大多数模型的优势是：能够处理超大超高维的数据集，并且**可辨识特征的重要性**\n",
    "3. 可有效估计缺失值，在存在大量缺失值情况下能保持准确率\n",
    "4. RF采用随机有放回的采样技术，理论上有1/3的数据未被用来训练，可用这部分数据来做验证，省去了专门划分验证集的步骤。该方法叫做“out-of-bag validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest的缺点\n",
    "1. 对回归问题处理的不好，因为天然地没法给出连续的预测值\n",
    "2. 黑盒子模型，能对其控制的参数较少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
